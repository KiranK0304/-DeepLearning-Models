# âœï¸ nanoGPT: Character-Level Transformer on Shakespeare

This project is a **from-scratch implementation** of Andrej Karpathyâ€™s [nanoGPT](https://github.com/karpathy/nanoGPT) using **PyTorch tensors and autograd only**â€”no high-level modules like `nn.Transformer` or `nn.MultiheadAttention`.

It trains a **character-level language model** on Shakespeareâ€™s text and generates eerily coherent English, proving that even small transformer models can capture deep language structure.

---

## ğŸ¯ Goal

To **truly understand** how GPT-style transformers workâ€”not just in theory, but by building every component manually. From how attention flows to how gradients train the model, this project was about mastering the **mechanics of intelligence**.

---

## ğŸ§  What I Learned

Through this deep dive, I gained **hands-on insight** into each core component of a Transformer:

### ğŸ”‘ Self-Attention (Q Â· Káµ€ â†’ Scores â†’ Softmax â†’ V)
- Built using matrix operations with fused Q, K, V projections.
- Understood how **attention scores** are formed by taking the dot product between Query and Key vectors.
- Softmax on QÂ·Káµ€ scales focus: some tokens "attend" more based on context.
- These scores weight the Value matrix, extracting the right blend of past token information.

### ğŸ§  Multi-Head Attention (MHA)
- Instead of looping, all heads were processed in parallel using tensor reshapingâ€”just like modern efficient implementations.
- One large matrix projects input into multiple QKV heads, then we concatenate and project back.
  
### ğŸ” Skip Connections + LayerNorm
- Skip connections carry a **residual stream**â€”the core idea in transformers.
- LayerNorm ensures each layer has stable activations and gradients, promoting smooth learning.

### ğŸ”’ Decoder Masking
- Used masking to prevent a token from attending to future tokensâ€”critical for autoregressive (left-to-right) training.

---

## ğŸ› ï¸ Manual Implementation Details

- âœ… Used only `torch.tensor`, `autograd`, `F.cross_entropy`, and `softmax`
- âœ… Built `LayerNorm` from scratch
- âœ… Built `MultiHeadAttention` manually (fused QKV, multiple heads in a single matrix)
- âœ… Created our own `Adam` optimizer using Python classes
- âœ… No PyTorch `nn.Module` or pre-built layersâ€”**raw and real**

---

## ğŸ“‚ File Structure

```bash
.
â”œâ”€â”€ gpt_from_scratch.ipynb     # Full transformer implementation and training
â””â”€â”€ README.md                  # You're here

