{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60976b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151cf735",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a588cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d40fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# mapping\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for i,s in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b7551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "data = torch.tensor(encode(text), dtype= torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 16\n",
    "batch_size = 32\n",
    "eval_iters = 100\n",
    "\n",
    "def get_batchsplit(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3a1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, _in, _out, bias=False):\n",
    "        self.w = torch.randn(_in, _out) * _in ** -0.5\n",
    "        self.b = torch.zeros(_out) if bias else None\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = x @ self.w\n",
    "        if self.b is not None:\n",
    "            out += self.b\n",
    "        return out\n",
    "\n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.w] + ([] if self.b is None else [self.b])\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "class MultiHeadAttention:\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = embed_dim // num_heads\n",
    "\n",
    "        # one big projection for all QKV together\n",
    "        self.qkv = Linear(embed_dim, 3* embed_dim) # for all head at once\n",
    "        self.out_proj = Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        # step 1: project and split into q,k,v\n",
    "        # C = num_heads * head_size\n",
    "        qkv = self.qkv(x) # (B, T, 3*C)\n",
    "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_size)\n",
    "\n",
    "        # we separate in the axis of qkv to index them easily\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, B, n_head, T, head_size)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # each: (B, n_heads, T, head_size)\n",
    "\n",
    "        # step 2: compute attention score\n",
    "        attn_scores = (q @ k.transpose(-1,-2)) / (self.head_size ** 0.5)\n",
    "        # mask = torch.tril(torch.ones(T,T), device=attn_scores.device).unsqueeze(0).unsqueeze(0)\n",
    "        mask = torch.ones(T, T, device=attn_scores.device).tril().unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        out = attn_probs @ v\n",
    "        out = out.transpose(1,2).reshape(B,T,C)\n",
    "\n",
    "        return self.out_proj(out)\n",
    "\n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [*self.qkv.parameters(), *self.out_proj.parameters()]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "class LayerNorm:\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        self.eps = 1e-8\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xmean = x.mean(-1, keepdim=True)\n",
    "        xvar = x.var(-1, keepdim=True)\n",
    "        return self.gamma * (x - xmean) / (torch.sqrt(xvar + self.eps)) + self.beta\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "class ReLU:\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.clamp(x,min=0.0)\n",
    "\n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "       return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "class FeedForward:\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        self.net = Sequential(\n",
    "            Linear(dim, 4*dim),\n",
    "            ReLU(),\n",
    "            Linear(4*dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.net.parameters()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "class TransformerBlock:\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        self.ln1 = LayerNorm(emb_dim)\n",
    "        self.attn = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ln2 = LayerNorm(emb_dim)\n",
    "        self.ff = FeedForward(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pre-norm and residual for attention\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # pre-norm and residual for feedforward\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return (\n",
    "            self.ln1.parameters() + self.attn.parameters() +\n",
    "            self.ln2.parameters() + self.ff.parameters()\n",
    "            )\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "class Embedding:\n",
    "\n",
    "    def __init__(self, num_emb, emb_dim):\n",
    "        self.table = torch.randn(num_emb, emb_dim)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        return self.table[idx]\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        return self.forward(idx)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.table]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "class PositionalEmbedding:\n",
    "\n",
    "    def __init__(self, max_len, emb_dim):\n",
    "        self.pe = torch.randn(max_len, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T = x.shape\n",
    "        return self.pe[:T]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.pe]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "class GPT:\n",
    "\n",
    "    def __init__(self, emb_dim, vocab_size, block_size, num_heads, num_layers):\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding = Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding = PositionalEmbedding(block_size, emb_dim)\n",
    "        self.blocks = [TransformerBlock(emb_dim, num_heads) for _ in range(num_layers)]\n",
    "        self.ln_f = LayerNorm(emb_dim)\n",
    "        self.lm_head = Linear(emb_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        B,T = idx.shape\n",
    "        x = self.token_embedding(idx) + self.position_embedding(idx)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        return self.forward(idx)\n",
    "\n",
    "    def parameters(self):\n",
    "        return (\n",
    "            self.token_embedding.parameters() + \n",
    "            self.position_embedding.parameters() +\n",
    "            [p for block in self.blocks for p in block.parameters()] +\n",
    "            self.ln_f.parameters() + \n",
    "            self.lm_head.parameters()\n",
    "        )\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is the array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]  \n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9714ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, parameters, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "        self.beta1 = betas[0]\n",
    "        self.beta2 = betas[1]\n",
    "        self.eps = eps\n",
    "        self.m = [torch.zeros_like(p) for p in self.parameters]  # First moment\n",
    "        self.v = [torch.zeros_like(p) for p in self.parameters]  # Second moment\n",
    "        self.t = 0  # timestep\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, p in enumerate(self.parameters):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = p.grad\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g * g)\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update param\n",
    "            p.data -= self.lr * m_hat / (v_hat.sqrt() + self.eps)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c06ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "batch_size = 32\n",
    "emb_dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "eval_iters = 100\n",
    "eval_interval = 500\n",
    "max_iters = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b109c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659392\n"
     ]
    }
   ],
   "source": [
    "model = GPT(\n",
    "        emb_dim=emb_dim,\n",
    "        vocab_size=vocab_size, \n",
    "        num_heads=num_heads,\n",
    "        block_size=block_size, \n",
    "        num_layers=num_layers,\n",
    ")\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_()\n",
    "    p.data = p.data.to(device)\n",
    "\n",
    "def count_parameters(params):\n",
    "    return sum(p.numel() for p in params)\n",
    "\n",
    "total_params = count_parameters(model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d656a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss: 2.5068\n",
      "Iter 200, Loss: 2.4745\n",
      "Iter 400, Loss: 2.3945\n",
      "Iter 600, Loss: 2.2893\n",
      "Iter 800, Loss: 2.1912\n",
      "Iter 1000, Loss: 2.0636\n",
      "Iter 1200, Loss: 1.9769\n",
      "Iter 1400, Loss: 1.8761\n",
      "Iter 1600, Loss: 1.8469\n",
      "Iter 1800, Loss: 1.7397\n",
      "Iter 2000, Loss: 1.6389\n",
      "Iter 2200, Loss: 1.6444\n",
      "Iter 2400, Loss: 1.5863\n",
      "Iter 2600, Loss: 1.5659\n",
      "Iter 2800, Loss: 1.5383\n",
      "Iter 3000, Loss: 1.5020\n",
      "Iter 3200, Loss: 1.4986\n",
      "Iter 3400, Loss: 1.4500\n",
      "Iter 3600, Loss: 1.4509\n",
      "Iter 3800, Loss: 1.4364\n",
      "Iter 4000, Loss: 1.4685\n",
      "Iter 4200, Loss: 1.4185\n",
      "Iter 4400, Loss: 1.3815\n",
      "Iter 4600, Loss: 1.3893\n",
      "Iter 4800, Loss: 1.3916\n",
      "Iter 5000, Loss: 1.3976\n",
      "Iter 5200, Loss: 1.3412\n",
      "Iter 5400, Loss: 1.3700\n",
      "Iter 5600, Loss: 1.3192\n",
      "Iter 5800, Loss: 1.3399\n",
      "Iter 6000, Loss: 1.2879\n",
      "Iter 6200, Loss: 1.2491\n",
      "Iter 6400, Loss: 1.2903\n",
      "Iter 6600, Loss: 1.2709\n",
      "Iter 6800, Loss: 1.2697\n",
      "Iter 7000, Loss: 1.3162\n",
      "Iter 7200, Loss: 1.2563\n",
      "Iter 7400, Loss: 1.2304\n",
      "Iter 7600, Loss: 1.2557\n",
      "Iter 7800, Loss: 1.2609\n",
      "Iter 8000, Loss: 1.2458\n",
      "Iter 8200, Loss: 1.2561\n",
      "Iter 8400, Loss: 1.2394\n",
      "Iter 8600, Loss: 1.2108\n",
      "Iter 8800, Loss: 1.2190\n",
      "Iter 9000, Loss: 1.2297\n",
      "Iter 9200, Loss: 1.1998\n",
      "Iter 9400, Loss: 1.1864\n",
      "Iter 9600, Loss: 1.2125\n",
      "Iter 9800, Loss: 1.1727\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "for iter in range(10000):\n",
    "    # Sample a batch\n",
    "    x,y = get_batchsplit('train')  # returns (x, y), with shape [B, T]\n",
    "\n",
    "    # Forward\n",
    "    logits = model(x)\n",
    "    # print('y shape: ',y.shape)\n",
    "    # print('logits shape: ',logits.shape)\n",
    "    B,T,C = logits.shape\n",
    "    logits = logits.view(B*T,C)\n",
    "    y = y.view(B*T)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    losses.append(loss)\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % 200 == 0:\n",
    "        print(f\"Iter {iter}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e41fe89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If I detem way: but it not yet,\n",
      "Though and mah outral of mile even my height.\n",
      "\n",
      "LEONTES:\n",
      "From heart and her honour more!\n",
      "I' do up't: but your into would ado,\n",
      "Mistress to accompy we where these fairies\n",
      "Which have the usurper knew of all little not.\n",
      "Seek how I heard thee use 'timonsted's, prevented\n",
      "Making the patricious for a facest\n",
      "Must of nice pursue of himself; if these prime in\n",
      "This enough numberfullness, is dishinings till\n",
      "To this billout and my noble sole out\n",
      "The moves bloody of the traitor o\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8881f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbef4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
